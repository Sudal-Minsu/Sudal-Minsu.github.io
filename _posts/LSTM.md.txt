```python
import pandas as pd
import numpy as np

data = pd.read_csv("dataset.csv")
data = data.dropna()
```


```python
# 감정 → 숫자로 매핑
sentiment_mapping = {
    'Extreme Fear': 1,
    'Fear': 2,
    'Neutral': 3,
    'Greed': 4,
    'Extreme Greed': 5,
}

data['Value_Classification'] = data['Value_Classification'].map(sentiment_mapping)
```


```python
data['BTC_Volume'] = np.log(data[['BTC_Volume']])
data['BTC_Closing'] = np.log(data['BTC_Closing'])
```


```python
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Value</th>
      <th>Value_Classification</th>
      <th>BTC_Closing</th>
      <th>BTC_Volume</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018-02-01</td>
      <td>30.0</td>
      <td>2</td>
      <td>9.123751</td>
      <td>23.021783</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018-02-02</td>
      <td>15.0</td>
      <td>1</td>
      <td>9.085995</td>
      <td>23.266984</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018-02-03</td>
      <td>40.0</td>
      <td>2</td>
      <td>9.124228</td>
      <td>22.706168</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018-02-04</td>
      <td>24.0</td>
      <td>1</td>
      <td>9.021237</td>
      <td>22.679628</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018-02-05</td>
      <td>11.0</td>
      <td>1</td>
      <td>8.847255</td>
      <td>22.951697</td>
    </tr>
  </tbody>
</table>
</div>




```python
features = ['Value','Value_Classification','BTC_Closing','BTC_Volume'] 

# 시퀀스 데이터 생성 함수
def build_sequence_dataset(df, seq_length):
    dataX = []
    dataY = []
    for i in range(0, len(df) - seq_length):
        _x = df.iloc[i:i+seq_length].values # 시퀀스 데이터
        _y = df.iloc[i + seq_length]['BTC_Closing']  # 다음 포인트의 기온을 레이블로 사용
        dataX.append(_x)
        dataY.append(_y)
    return np.array(dataX), np.array(dataY)

# 시퀀스 길이 정의
seq_length = 7  # 과거 7일의 데이터를 기반으로 다음날의 가격을 예측
# 데이터셋 생성
sequence_dataX, sequence_dataY = build_sequence_dataset(data[features], seq_length)
```


```python
# 생성된 시퀀스 데이터의 첫 번째 요소를 출력하여 실습 예제 확인
print(f"sequence_dataX 형태 : {sequence_dataX.shape}")
print(f"sequence_dataX 개수 : {len(sequence_dataX)}")

print("첫번째 sequenceX:")
print(sequence_dataX[0])
print("첫번째 sequenceY:")
print(sequence_dataY[0])
```

    sequence_dataX 형태 : (1875, 7, 4)
    sequence_dataX 개수 : 1875
    첫번째 sequenceX:
    [[30.          2.          9.12375146 23.02178271]
     [15.          1.          9.08599523 23.26698368]
     [40.          2.          9.12422788 22.70616758]
     [24.          1.          9.02123704 22.67962829]
     [11.          1.          8.84725493 22.95169726]
     [ 8.          1.          8.95596412 23.3623089 ]
     [36.          2.          8.93870221 22.9391246 ]]
    첫번째 sequenceY:
    9.019856374193836
    


```python
import torch
import torch.nn as nn

num_features = len(features)
num_hidden = 10 

# LSTM 모듈 초기화
model_lstm = nn.LSTM(input_size=num_features, hidden_size=num_hidden, num_layers=1)
print("모델 구조:")
print(model_lstm)
```

    모델 구조:
    LSTM(4, 10)
    

히든 사이즈(메모리 용량, 클수록 계산비용 증가) 랑 레이어(클수록 더 복잡한 패턴 학습 가능) 수 생각하기


```python
for name, param in model_lstm.named_parameters():
    print(f"{name}: {param.shape}")

# 특정 가중치에 직접 접근

print("weight_ih_l0 shape:", model_lstm.weight_ih_l0.shape)
print("weight_hh_l0 shape:", model_lstm.weight_hh_l0.shape)
print("bias_ih_l0 shape:", model_lstm.bias_ih_l0.shape)
print("bias_hh_l0 shape:", model_lstm.bias_hh_l0.shape)
```

    weight_ih_l0: torch.Size([40, 4])
    weight_hh_l0: torch.Size([40, 10])
    bias_ih_l0: torch.Size([40])
    bias_hh_l0: torch.Size([40])
    weight_ih_l0 shape: torch.Size([40, 4])
    weight_hh_l0 shape: torch.Size([40, 10])
    bias_ih_l0 shape: torch.Size([40])
    bias_hh_l0 shape: torch.Size([40])
    


```python
#[레이어의 개수, 배치의 개수, 은닉 유닛의 개수]
h0 = torch.zeros(1, 1, num_hidden)
c0 = torch.zeros(1, 1, num_hidden)

print("Hidden state shape:", h0.shape)
print("Cell state shape:", c0.shape)
```

    Hidden state shape: torch.Size([1, 1, 10])
    Cell state shape: torch.Size([1, 1, 10])
    

배치의 개수(한번에 학습할 데이터의 수) 생각하기


```python
# 시퀀스 데이터를 텐서로 변환
sequence_data_tensor = torch.tensor(sequence_dataX, dtype=torch.float32)

print(f"sequence_dataX의 형태 : {sequence_dataX.shape}")
print(f"sequence_data_tensor의 형태 : {sequence_data_tensor.shape}")
print(f"sequence_data_tensor[0]의 형태 : {sequence_data_tensor[0].shape}")

display(sequence_data_tensor[0])
```

    sequence_dataX의 형태 : (1875, 7, 4)
    sequence_data_tensor의 형태 : torch.Size([1875, 7, 4])
    sequence_data_tensor[0]의 형태 : torch.Size([7, 4])
    


    tensor([[30.0000,  2.0000,  9.1238, 23.0218],
            [15.0000,  1.0000,  9.0860, 23.2670],
            [40.0000,  2.0000,  9.1242, 22.7062],
            [24.0000,  1.0000,  9.0212, 22.6796],
            [11.0000,  1.0000,  8.8473, 22.9517],
            [ 8.0000,  1.0000,  8.9560, 23.3623],
            [36.0000,  2.0000,  8.9387, 22.9391]])



```python
#입력데이터로 (시퀀스 갯수, 배치 갯수, 피처 갯수 )를 기대함
batch_added_tensor = sequence_data_tensor[0].unsqueeze(1)
print(f"배치 차원 추가한 시퀀스 데이터 : {batch_added_tensor.shape}")
```

    배치 차원 추가한 시퀀스 데이터 : torch.Size([7, 1, 4])
    


```python
tot_num_sequence_data = sequence_data_tensor.size(0)

for i in range(tot_num_sequence_data):
    # 현재 시퀀스 선택 및 차원 추가 (seq_len, batch, input_size)
    current_sequence = sequence_data_tensor[i].unsqueeze(1)  # 배치 차원 추가

    # LSTM 네트워크를 통한 시퀀스 처리
    output, (hn, cn) = model_lstm(current_sequence, (h0, c0))

 # 결과 출력 (첫 번째, 마지막, 매 100번째 시퀀스만 출력)
    if i == 0 or i == sequence_data_tensor.size(0) - 1:
        print(f"Sequence {i+1} Output shape:", output.shape)
        print(f"Sequence {i+1} Hidden state shape:", hn.shape)
        print(f"Sequence {i+1} Cell state shape:", cn.shape)
        
    h0, c0 = hn, cn 

```

    Sequence 1 Output shape: torch.Size([7, 1, 10])
    Sequence 1 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 1 Cell state shape: torch.Size([1, 1, 10])
    Sequence 1875 Output shape: torch.Size([7, 1, 10])
    Sequence 1875 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 1875 Cell state shape: torch.Size([1, 1, 10])
    


```python

```
